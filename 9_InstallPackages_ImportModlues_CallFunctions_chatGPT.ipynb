{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tunip0067/NLP_2023/blob/main/9_InstallPackages_ImportModlues_CallFunctions_chatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ Python Library\n",
        "  - Python modues ì„ ê³„ì¸µì ì¸ ë””ë ‰í† ë¦¬ í˜•íƒœë¡œ êµ¬ì„±\n",
        "  - **!pip**: Package manager\n",
        "  - **!pip install <font color = 'red'> NameLibrary**\n",
        "> A <font color = 'red'> **Python library**</font> refers to a collection of modules or functions that provide specific functionality, often focused on a particular domain or purpose. Libraries can be used to extend the capabilities of Python by providing pre-written code that can be imported and used in your own programs. Examples of popular Python libraries include NumPy for numerical computing, pandas for data manipulation and analysis, and requests for making HTTP requests. On the other hand, a <font color = 'blue'> **Python package**</font> is a way of organizing related modules into a directory hierarchy. A package is essentially a directory that contains one or more Python module files, along with an optional __init__.py file that signifies it as a package. Packages help to organize and structure large codebases by grouping related functionality together. They can also contain sub-packages, creating a nested structure.\n",
        "\n",
        "# ğŸ’ğŸ’ Python Moduess\n",
        "  - Python functions ë¡œ êµ¬ì„±\n",
        "  - **from í˜í‚¤ì§€ì´ë¦„ import ëª¨ë“ˆì´ë¦„**\n",
        "  - **import** **í˜í‚¤ì§€.ëª¨ë“ˆì´ë¦„ <font color='green'>[ì™¸ë¶€ í˜í‚¤ì§€ ê²½ìš°]**</font>\n",
        "  - **import ëª¨ë“ˆì´ë¦„ <font color='purple'>[Python ë‚´ì¥ í˜í‚¤ì§€ ê²½ìš°]**</font>\n",
        "  - **import ëª¨ë“ˆì´ë¦„ as Abbreviation (ì¶•ì•½ì–´)**\n",
        "\n",
        "\n",
        "# ğŸ€ âš½ âš¾ ğŸ¾ Python functions\n",
        "  - ì™¸ë¶€ ëª¨ë“ˆì— ìˆëŠ” í•¨ìˆ˜\n",
        "  - import ëª¨ë“ˆì´ë¦„.í•¨ìˆ˜ì´ë¦„()\n",
        "  - from ëª¨ë“ˆì´ë¦„ import í•¨ìˆ˜ì´ë¦„\n",
        "\n",
        "### For you information, check out **Python Module Index**\n",
        "* [Visit Colab documentation]((https://docs.python.org/3/py-modindex.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "aCsaIGXK73ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<font color = 'purple'> **ğŸ‘€ Install Python Libraries** â¤µï¸"
      ],
      "metadata": {
        "id": "nR3MiBaAM--d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ¹ ğŸ‘€ ğŸ¾ The **Pandas** is a popular open-source library for data manipulation and analysis. It offers data structures like **Series and DataFrame for handling structured data**. With powerful functionalities, it enables tasks such as <font color = 'red'>**indexing, filtering, grouping, and merging data**</font>. Pandas supports various file formats and integrates well with other libraries like NumPy and Matplotlib. It provides an intuitive and efficient way to work with large datasets in Python.\n",
        "\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "9olyrXb-H-mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas # !pip installì„ ê¹Œë¨¹ì§€ ë§ì"
      ],
      "metadata": {
        "id": "vBQVJb5c56Xf",
        "outputId": "807b3c39-442b-4ef6-9684-9e142ee308d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ¹ ğŸ‘€ ğŸ¾ The **scikit-learn**, often referred to as sklearn, is a widely used **machine learning library**. It provides a comprehensive collection of tools and algorithms for various machine learning tasks such as <font color = 'red'>**classification, regression, clustering, and dimensionality reduction**</font>. With a consistent and user-friendly API, scikit-learn simplifies the process of building machine learning models. It supports data preprocessing, feature selection, model evaluation, and model tuning. The library also offers helpful utilities for handling datasets and implementing machine learning workflows. Overall, scikit-learn is a valuable resource for both beginners and experienced practitioners in the field of machine learning.\n",
        "\n",
        "!pip install scikit-learn #corpus-toolkit íŒ¨í‚¤ì§€ì— í•˜ì´í”ˆ ìˆìŒ"
      ],
      "metadata": {
        "id": "sD-yK0LIICxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "53zUjvs87yxb",
        "outputId": "2c8a756e-ad70-4f97-9e1c-581ba2c0aece",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ¹ ğŸ‘€ ğŸ¾ The **Matplotlib** is a popular plotting library for Python. It provides a flexible and comprehensive set of tools for creating various types of plots and visualizations. With a simple and intuitive interface, Matplotlib allows customization of plot appearance, axes, labels, and styles. It supports <font color = 'red'>**line plots, scatter plots, bar charts, histograms**</font>, and more. Matplotlib *integrates well with NumPy and Pandas for data manipulation and analysis*. It is widely used for data exploration, presentation, and publication-quality visualizations in scientific computing and data analysis.\n",
        "\n",
        "!pip install matplot"
      ],
      "metadata": {
        "id": "Yo1rFBHOIOrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplot"
      ],
      "metadata": {
        "id": "vdvp876i91TG",
        "outputId": "b66ff9ea-cf46-4b7f-cffe-b44b04251091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplot\n",
            "  Downloading matplot-0.1.9-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting pyloco>=0.0.134 (from matplot)\n",
            "  Downloading pyloco-0.0.139-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m701.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from matplot) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->matplot) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->matplot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->matplot) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->matplot) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->matplot) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->matplot) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->matplot) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->matplot) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->matplot) (2.8.2)\n",
            "Collecting ushlex (from pyloco>=0.0.134->matplot)\n",
            "  Downloading ushlex-0.99.1.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from pyloco>=0.0.134->matplot) (1.6.4)\n",
            "Collecting twine (from pyloco>=0.0.134->matplot)\n",
            "  Downloading twine-4.0.2-py3-none-any.whl (36 kB)\n",
            "Collecting typing (from pyloco>=0.0.134->matplot)\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting SimpleWebSocketServer (from pyloco>=0.0.134->matplot)\n",
            "  Downloading SimpleWebSocketServer-0.1.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->matplot) (1.16.0)\n",
            "Collecting pkginfo>=1.8.1 (from twine->pyloco>=0.0.134->matplot)\n",
            "  Downloading pkginfo-1.9.6-py3-none-any.whl (30 kB)\n",
            "Collecting readme-renderer>=35.0 (from twine->pyloco>=0.0.134->matplot)\n",
            "  Downloading readme_renderer-42.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from twine->pyloco>=0.0.134->matplot) (2.31.0)\n",
            "Collecting requests-toolbelt!=0.9.0,>=0.8.0 (from twine->pyloco>=0.0.134->matplot)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from twine->pyloco>=0.0.134->matplot) (2.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.10/dist-packages (from twine->pyloco>=0.0.134->matplot) (6.8.0)\n",
            "Requirement already satisfied: keyring>=15.1 in /usr/lib/python3/dist-packages (from twine->pyloco>=0.0.134->matplot) (23.5.0)\n",
            "Collecting rfc3986>=1.4.0 (from twine->pyloco>=0.0.134->matplot)\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from twine->pyloco>=0.0.134->matplot) (13.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=3.6->twine->pyloco>=0.0.134->matplot) (3.17.0)\n",
            "Collecting nh3>=0.2.14 (from readme-renderer>=35.0->twine->pyloco>=0.0.134->matplot)\n",
            "  Downloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docutils>=0.13.1 in /usr/local/lib/python3.10/dist-packages (from readme-renderer>=35.0->twine->pyloco>=0.0.134->matplot) (0.18.1)\n",
            "Requirement already satisfied: Pygments>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from readme-renderer>=35.0->twine->pyloco>=0.0.134->matplot) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->twine->pyloco>=0.0.134->matplot) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->twine->pyloco>=0.0.134->matplot) (0.1.2)\n",
            "Building wheels for collected packages: SimpleWebSocketServer, typing, ushlex\n",
            "  Building wheel for SimpleWebSocketServer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SimpleWebSocketServer: filename=SimpleWebSocketServer-0.1.2-py3-none-any.whl size=9663 sha256=614acb78f1f83e386449835b2afe15ce7db102840023bdd77fd501f12ed4caba\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/cb/9d/23ef01c9c26b978be6d29ed456d7348abd0e96b1563049bd63\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26308 sha256=8caa9aa8b7d4f0e6bca60a4deedb299fdc446ef43f28ceaaea6729d5e162efba\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
            "  Building wheel for ushlex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ushlex: filename=ushlex-0.99.1-py3-none-any.whl size=4393 sha256=a66281d07f1dfd2da31df21945f9c3ea1b030f9fd6d904ab036e068a55d5b44a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/e4/80/4de44668798392eb2f5b24f0130df2f5d28b43cb256bf9c1f9\n",
            "Successfully built SimpleWebSocketServer typing ushlex\n",
            "Installing collected packages: ushlex, SimpleWebSocketServer, nh3, typing, rfc3986, readme-renderer, pkginfo, requests-toolbelt, twine, pyloco, matplot\n",
            "Successfully installed SimpleWebSocketServer-0.1.2 matplot-0.1.9 nh3-0.2.14 pkginfo-1.9.6 pyloco-0.0.139 readme-renderer-42.0 requests-toolbelt-1.0.0 rfc3986-2.0.0 twine-4.0.2 typing-3.7.4.3 ushlex-0.99.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ¹ ğŸ‘€ ğŸ¾ The **NLTK** (Natural Language Toolkit) is a powerful library for natural language processing (NLP) tasks. It provides tools and resources for tasks like <font color = 'red'>**tokenization, stemming, tagging, parsing, and sentiment analysis**</font>. NLTK offers a wide range of corpora and lexical resources for linguistic analysis. It supports **text classification, text generation, and language modeling**. NLTK includes pre-trained models and algorithms for various NLP tasks, making it suitable for both beginners and advanced users in the field of NLP. Overall, NLTK is a valuable resource for working with *human language data and performing NLP tasks in Python*.\n",
        "\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "LKUrjZgzza9Q",
        "outputId": "f39920f0-4811-457f-8df3-a1f1eba58042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "DV9qe8ut9xAG",
        "outputId": "7e113b71-1315-4384-f191-9d1adeba5376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ¹ ğŸ‘€ ğŸ¾ **Corpus-toolkit** package grew out of courses in corpus linguistics and learner corpus research. The toolkit attempts to balance simplicity of use, broad application, and scalability. Common corpus analyses such as <font color = 'red'>**the calculation of word and n-gram frequency and range, keyness, and collocation**</font> are included. In addition, more advanced analyses such as the **identification of dependency bigrams (e.g., verb-direct object combinations) and their frequency, range, and strength of association** are also included.\n",
        "\n",
        "!pip install corpus-toolkit #corpus-toolkit íŒ¨í‚¤ì§€ì— í•˜ì´í”ˆ ìˆìŒ"
      ],
      "metadata": {
        "id": "aFjO_wBAH5Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install corpus-toolkit"
      ],
      "metadata": {
        "id": "cwOZkY1p-rRF",
        "outputId": "2e6bd923-a064-44da-96b8-e0a05936fb6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting corpus-toolkit\n",
            "  Downloading corpus_toolkit-0.32-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: corpus-toolkit\n",
            "Successfully installed corpus-toolkit-0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color = 'brown'> Student's activity** â¤µï¸\n",
        "\n",
        "### Exercise for <font color = 'red'> installing nltk package, importing its module, and calling its functions\n",
        "\n",
        "**NLTK: Python library**\n",
        "* You are correct! When using **Google Colab**, certain libraries, including **NLTK**, are **pre-installed and available for immediate use** without the need for additional installation. This is because Colab provides a pre-configured environment with several popular libraries and modules already installed, allowing you to import and use them directly in your code. So, in the case of using Colab, you can indeed use the NLTK module without explicitly installing the NLTK library.\n",
        ">\n",
        "* NLTKì˜ ê¸°ëŠ¥ì„ ì œëŒ€ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” NLTK Dataë¼ëŠ” ì—¬ëŸ¬ ë°ì´í„°ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ì„¤ì¹˜í•´ì•¼ í•œë‹¤.\n",
        ">\n",
        "* ì´ë¥¼ ìœ„í•´ì„œ íŒŒì´ì¬ ì½”ë“œ ë‚´ì—ì„œ import nltk ì´í›„ì— nltk.download()ë¼ëŠ” ì½”ë“œë¥¼ ìˆ˜í–‰í•˜ì—¬ ì„¤ì¹˜í•œë‹¤.\n",
        ">\n",
        "* **Reference**\n",
        "  * [wikidocs](https://wikidocs.net/22488)\n"
      ],
      "metadata": {
        "id": "SL1F8WCDKvoN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TPWVgk272SH"
      },
      "outputs": [],
      "source": [
        "!pip install nltk #This step can be skipped since it is pre-installed on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "7dBxHVDoIub9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "kjJG73tG_B8J",
        "outputId": "f2fb6f6f-0792-451a-f3f9-590fc6c67d00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color = 'brown'> Student's activity** â¤µï¸\n",
        "\n",
        "### Exercise for <font color = 'red'> *importing os module and calling its functions*\n",
        "\n",
        "   -The **os module is a built-in module in Python**, meaning it is available by default in any Python installation. You don't need to install it separately or use any package manager. The os module provides functions for interacting with the operating system, such as accessing files and directories, managing processes, and other system-related tasks. You can import and use the os module in your Python programs without any additional installation steps."
      ],
      "metadata": {
        "id": "hioxpHpfQLjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os                 #Python built-in os module ë¶ˆëŸ¬ ë“¤ì´ê¸°\n",
        "os.mkdir (\"txtfolder\")    #os ëª¨ë“ˆê³¼ mkdir í•¨ìˆ˜ ì‚¬ì´ì— period ìˆìŒ. Check \"txtfolder\" under Files of Colab."
      ],
      "metadata": {
        "id": "8PxL2p05WDin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.mkdir (\"txtfolder\")"
      ],
      "metadata": {
        "id": "UjfOCp0d_vC8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "text = \"Python programming is a high-level, interpreted programming language known for its simplicity and readability. It emphasizes code readability with its clean syntax, making it easier to write and understand. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It has a vast standard library and a thriving ecosystem of third-party libraries and frameworks, making it suitable for various domains such as web development, data analysis, machine learning, and automation. Python's versatility, ease of use, and extensive community support have contributed to its popularity among developers of all skill levels.\"\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence = sent_tokenize(text)\n",
        "print('ë¬¸ì¥ í† í°í™”: %s' %sentence)"
      ],
      "metadata": {
        "id": "PcbpiNjAAsI6",
        "outputId": "b7de5052-3200-4211-adb2-b0fdfef27e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "ë¬¸ì¥ í† í°í™”: ['Python programming is a high-level, interpreted programming language known for its simplicity and readability.', 'It emphasizes code readability with its clean syntax, making it easier to write and understand.', 'Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.', 'It has a vast standard library and a thriving ecosystem of third-party libraries and frameworks, making it suitable for various domains such as web development, data analysis, machine learning, and automation.', \"Python's versatility, ease of use, and extensive community support have contributed to its popularity among developers of all skill levels.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "text = \"Python programming is a high-level, interpreted programming language known for its simplicity and readability. It emphasizes code readability with its clean syntax, making it easier to write and understand. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It has a vast standard library and a thriving ecosystem of third-party libraries and frameworks, making it suitable for various domains such as web development, data analysis, machine learning, and automation. Python's versatility, ease of use, and extensive community support have contributed to its popularity among developers of all skill levels.\"\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence = sent_tokenize(text)\n",
        "print('ë¬¸ì¥ í† í°í™”: %s' %sentence)\n"
      ],
      "metadata": {
        "id": "b0VovEVr2dFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ¹ ğŸ‘€ ğŸ¾ **Lexical-diversity** library is a Python package that provides tools and functions for analyzing the lexical diversity of text.  i) <font color = 'red'>**Type-Token Ratio (TTR)**</font> measures the proportion of unique words (types) in a text compared to the total number of words (tokens). It provides insights into vocabulary richness. ii)  <font color = 'red'>**Moving Standardized Type-Token Ratio (MSTTR)**</font> is a dynamic measure of lexical diversity that takes into account a moving window of text, allowing you to assess diversity over smaller sections of text. iii)  <font color = 'red'>**Moving Average Type-Token Ration (MATTR)**</font> calculates the Type-Token Ratio (TTR) within a sliding window as it moves through the text, and then computes the average of these TTR values over the entire text. The formula for MATTR: MATTR = (1 / N) * âˆ‘(TTR_i).\n",
        "\n",
        "#@markdown ğŸ¹ ğŸ‘€ ğŸ¾ Total number of words / Total number of type\n",
        "\n",
        "!pip install lexical-diversity"
      ],
      "metadata": {
        "id": "VBD9lDWz0Eoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexical-diversity"
      ],
      "metadata": {
        "id": "mwIy5RmaD-u0",
        "outputId": "b415f3b3-1ca6-4379-dec6-768510bd8776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lexical-diversity\n",
            "  Downloading lexical_diversity-0.1.1-py3-none-any.whl (117 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/117.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m112.6/117.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lexical-diversity\n",
            "Successfully installed lexical-diversity-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### â›” **Shall we write a scrip?**"
      ],
      "metadata": {
        "id": "Q76RwDl81-0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**[Sign up chat GPT](https://chat.openai.com)**\n",
        "\n",
        "After you have created a chat GPT account, visit **[minjung's github page](usercontent.com/ms624atyale/Temp_Data/main/TheAesop4Children_1stEpisode.txt)** and copy the content of the _TheAesop4Children_1stEpisode.txt_ file.\n",
        "\n",
        "1. Ask chatGPT to write a python script for lexical diversity, TTR, using the following text, \"COPY & PASTE YOUR TEXT.\"\n",
        "\n",
        "\n",
        "2. Ask again chatGPT to include the list of text, tokens, and types (i.e., unique words).\n",
        "\n",
        "3. Ask again to include converting tokens to lowercase.\n",
        "\n",
        "4. Ask again, does higher vlaue of TTR indicate either greater or lower variability of lexical use?"
      ],
      "metadata": {
        "id": "5ugkxTU69GS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Ask chatGPT to write a python script for lexical diversity, TTR, using the following text, \"COPY & PASTE YOUR TEXT.\""
      ],
      "metadata": {
        "id": "4hnHrakwLUab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text\n",
        "text = \"\"\"There was once a little Kid whose growing horns made him think he was a grown-up Billy Goat and able to take care of himself. So one evening when the flock started home from the pasture and his mother called, the Kid paid no heed and kept right on nibbling the tender grass. A little later when he lifted his head, the flock was gone. He was all alone. The sun was sinking. Long shadows came creeping over the ground. A chilly little wind came creeping with them making scary noises in the grass. The Kid shivered as he thought of the terrible Wolf. Then he started wildly over the field, bleating for his mother. But not half-way, near a clump of trees, there was the Wolf! The Kid knew there was little hope for him. â€œPlease, Mr. Wolf,â€ he said trembling, â€œI know you are going to eat me. But first please pipe me a tune, for I want to dance and be merry as long as I can.â€œThe Wolf liked the idea of a little music before eating, so he struck up a merry tune and the Kid leaped and frisked gaily. Meanwhile, the flock was moving slowly homeward. In the still evening air the Wolf's piping carried far. The Shepherd Dogs pricked up their ears. They recognized the song the Wolf sings before a feast, and in a moment they were racing back to the pasture. The Wolf's song ended suddenly, and as he ran, with the Dogs at his heels, he called himself a fool for turning piper to please a Kid, when he should have stuck to his butcher's trade. Do not let anything turn you from your purpose.\"\"\"\n",
        "\n",
        "# Tokenize the text into words (split by whitespace and punctuation)\n",
        "words = text.split()\n",
        "\n",
        "# Calculate TTR (Type-Token Ratio)\n",
        "unique_words = set(words)\n",
        "ttr = len(unique_words) / len(words)\n",
        "\n",
        "# Print the TTR\n",
        "print(\"Type-Token Ratio (TTR):\", ttr)"
      ],
      "metadata": {
        "id": "iPPOr3lc9lVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Ask again chatGPT to include the list of text, tokens, and types (i.e., unique words)."
      ],
      "metadata": {
        "id": "9gcC9G-APzEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text\n",
        "text = \"\"\"There was once a little Kid whose growing horns made him think he was a grown-up Billy Goat and able to take care of himself. So one evening when the flock started home from the pasture and his mother called, the Kid paid no heed and kept right on nibbling the tender grass. A little later when he lifted his head, the flock was gone. He was all alone. The sun was sinking. Long shadows came creeping over the ground. A chilly little wind came creeping with them making scary noises in the grass. The Kid shivered as he thought of the terrible Wolf. Then he started wildly over the field, bleating for his mother. But not half-way, near a clump of trees, there was the Wolf! The Kid knew there was little hope for him. â€œPlease, Mr. Wolf,â€ he said trembling, â€œI know you are going to eat me. But first please pipe me a tune, for I want to dance and be merry as long as I can.â€œThe Wolf liked the idea of a little music before eating, so he struck up a merry tune and the Kid leaped and frisked gaily. Meanwhile, the flock was moving slowly homeward. In the still evening air the Wolf's piping carried far. The Shepherd Dogs pricked up their ears. They recognized the song the Wolf sings before a feast, and in a moment they were racing back to the pasture. The Wolf's song ended suddenly, and as he ran, with the Dogs at his heels, he called himself a fool for turning piper to please a Kid, when he should have stuck to his butcher's trade. Do not let anything turn you from your purpose.\"\"\"\n",
        "\n",
        "# Tokenize the text into words (split by whitespace and punctuation)\n",
        "words = text.split()\n",
        "\n",
        "# Calculate TTR (Type-Token Ratio)\n",
        "unique_words = set(words)\n",
        "ttr = len(unique_words) / len(words)\n",
        "\n",
        "# Print the TTR, list of tokens, and list of unique words (types)\n",
        "print(\"Type-Token Ratio (TTR):\", ttr)\n",
        "print(\"Tokens:\", words)\n",
        "print(\"Unique words (Types):\", sorted(unique_words))"
      ],
      "metadata": {
        "id": "hH-xklk3_RRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Ask again to include converting tokens to lowercase."
      ],
      "metadata": {
        "id": "Lc5gV9DWP4Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to tokenize and convert to lowercase\n",
        "def tokenize_and_lower(text):\n",
        "    # Tokenize the text into words (split by whitespace and punctuation)\n",
        "    words = text.split()\n",
        "    # Convert words to lowercase\n",
        "    words_lower = [word.lower() for word in words]\n",
        "    return words_lower\n",
        "\n",
        "# Define the text\n",
        "text = \"\"\"There was once a little Kid whose growing horns made him think he was a grown-up Billy Goat and able to take care of himself. So one evening when the flock started home from the pasture and his mother called, the Kid paid no heed and kept right on nibbling the tender grass. A little later when he lifted his head, the flock was gone. He was all alone. The sun was sinking. Long shadows came creeping over the ground. A chilly little wind came creeping with them making scary noises in the grass. The Kid shivered as he thought of the terrible Wolf. Then he started wildly over the field, bleating for his mother. But not half-way, near a clump of trees, there was the Wolf! The Kid knew there was little hope for him. â€œPlease, Mr. Wolf,â€ he said trembling, â€œI know you are going to eat me. But first please pipe me a tune, for I want to dance and be merry as long as I can.â€œ The Wolf liked the idea of a little music before eating, so he struck up a merry tune and the Kid leaped and frisked gaily. Meanwhile, the flock was moving slowly homeward. In the still evening air the Wolf's piping carried far. The Shepherd Dogs pricked up their ears. They recognized the song the Wolf sings before a feast, and in a moment they were racing back to the pasture. The Wolf's song ended suddenly, and as he ran, with the Dogs at his heels, he called himself a fool for turning piper to please a Kid, when he should have stuck to his butcher's trade. Do not let anything turn you from your purpose.\"\"\"\n",
        "\n",
        "# Get the tokens in lowercase\n",
        "tokens_lower = tokenize_and_lower(text)\n",
        "\n",
        "# Calculate TTR (Type-Token Ratio)\n",
        "unique_words = set(tokens_lower)\n",
        "ttr = len(unique_words) / len(tokens_lower)\n",
        "\n",
        "# Print the TTR, list of tokens, and list of unique words (types)\n",
        "print(\"Type-Token Ratio (TTR):\", ttr)\n",
        "print(\"Tokens in Lowercase:\", tokens_lower)\n",
        "print(\"Unique words (Types):\", sorted(unique_words))"
      ],
      "metadata": {
        "id": "k88w62oEJx2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Ask again, does higher vlaue of TTR indicate either greater or lower variability of lexical use?"
      ],
      "metadata": {
        "id": "re2gErgHQDUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer from chat GPT: <font color = 'purple'> A higher value of TTR (Type-Token Ratio) typically indicates greater variability of lexical use. TTR measures the diversity of words in a given text. When TTR is higher, it means that there are more unique words (types) relative to the total number of words (tokens), suggesting a wider range of vocabulary and more diverse lexical usage in the text. In contrast, a lower TTR indicates that a smaller set of words is repeated more frequently, suggesting less diversity and greater repetition of words. So, a higher TTR is associated with a more varied lexical repertoire."
      ],
      "metadata": {
        "id": "B5euKG5JLG3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ‘€ <font color = 'red'> Do it yourself first!**  \n",
        "- âœ… Now **accomodate importing url** from your repository of Temp_Data on your github account **to** your current jupiter notebook on Colab, and calculate TTR again.\n",
        ">\n",
        "- âœ… You should estimate TTR with tokens converted lowercase!!!"
      ],
      "metadata": {
        "id": "3DttpqN3_nI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##**ğŸ¯ğŸ¯ In the following, you can find one possible answer.**\n",
        "\n",
        "def tokenize_and_lower(text):\n",
        "    # Tokenize the text into words (split by whitespace and punctuation)\n",
        "    words = text.split()\n",
        "    # Convert words to lowercase\n",
        "    words_lower = [word.lower() for word in words]\n",
        "    return words_lower\n",
        "\n",
        "import urllib.request\n",
        "url = \"https://raw.githubusercontent.com/ms624atyale/Temp_Data/main/TheAesop4Children_1stEpisode.txt\"\n",
        "response = urllib.request.urlopen(url)\n",
        "content = response.read().decode('utf-8')\n",
        "print(content)\n",
        "\n",
        "# Get the tokens in lowercase\n",
        "tokens_lower = tokenize_and_lower(text)\n",
        "\n",
        "# Calculate TTR (Type-Token Ratio)\n",
        "unique_words = set(tokens_lower)\n",
        "ttr = len(unique_words) / len(tokens_lower)\n",
        "\n",
        "# Print the TTR, list of tokens, and list of unique words (types)\n",
        "print(\"Type-Token Ratio (TTR):\", ttr)\n",
        "print(\"Tokens in Lowercase:\", tokens_lower)\n",
        "print(\"Unique words (Types):\", sorted(unique_words))\n"
      ],
      "metadata": {
        "id": "ECO8UP8rC2Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ğŸ‘€ <font color = 'red'> Now you are going to do...**  \n",
        "- Find an article on a web page of your choice.\n",
        "- This time, vist **[www.npr.org](https://www.npr.org/2023/11/08/1211483883/olympics-russia-israel-gaza-sanctions-ioc)** for class activity."
      ],
      "metadata": {
        "id": "M3mpBoU6H8wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### ğŸ Write a script for TTL as you copy and paste html format in the text.\n",
        "\n",
        "\n",
        "# Define a function to tokenize and convert to lowercase\n",
        "def tokenize_and_lower(text):\n",
        "    # Tokenize the text into words (split by whitespace and punctuation)\n",
        "    words = text.split()\n",
        "    # Convert words to lowercase\n",
        "    words_lower = [word.lower() for word in words]\n",
        "    return words_lower\n",
        "\n",
        "# Define the text as you copy and paste something from an internet webstie.\n",
        "text = \"\"\"\n",
        "When violence in Israel and Gaza escalated after last month's terror attack by Hamas, the International Olympic Committee (IOC) issued a statement warning against \"discriminatory behavior\" against Israeli athletes competing around the world.\\n\n",
        "\"[A]thletes cannot be held responsible for the actions of their governments,\" an IOC spokesperson told the German Press Agency, promising \"swift action\" if incidents occur.\n",
        "\"\"\"\n",
        "\n",
        "# Get the tokens in lowercase\n",
        "tokens_lower = tokenize_and_lower(text)\n",
        "\n",
        "# Calculate TTR (Type-Token Ratio)\n",
        "unique_words = set(tokens_lower)\n",
        "ttr = len(unique_words) / len(tokens_lower)\n",
        "\n",
        "# Print the TTR, list of tokens, and list of unique words (types)\n",
        "print(\"Type-Token Ratio (TTR):\", ttr)\n",
        "print(\"Tokens in Lowercase:\", tokens_lower)\n",
        "print(\"Unique words (Types):\", sorted(unique_words))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L3hnINcDRaZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### ğŸğŸ Write a script for TTL as you use url of your github where you've saved part of webpage using ms word as UTF-8.\n",
        "\n",
        "import urllib.request\n",
        "url = \"https://raw.githubusercontent.com/ms624atyale/Temp_Data/main/NPR_article_sample.txt\"\n",
        "response = urllib.request.urlopen(url)\n",
        "content = response.read().decode('utf-8')\n",
        "\n",
        "print(content)\n",
        "\n",
        "words = content.split()\n",
        "\n",
        "# Calculate TTR (Type-Token Ratio)\n",
        "unique_words = set(words)\n",
        "ttr = len(unique_words) / len(words)\n",
        "\n",
        "# Print the TTR, list of tokens, and list of unique words (types)\n",
        "print(\"Type-Token Ratio (TTR):\", ttr)\n",
        "print(\"Tokens:\", words)\n",
        "print(\"Unique words (Types):\", sorted(unique_words))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-S-xxJ6IJCgW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}